{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3022 Homework\n",
    "<figure>\n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you referenced any web sites or solutions not of your own creation, list those references here:\n",
    "\n",
    "* List any external references or resources here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import patsy\n",
    "import sklearn\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression uses the *logit* function to assign probabilities to predicted values. The parameters of the logit function are determined using maximum liklihood estimation. In certain cases, logistic classification fails to classify data we would consider \"easy\" to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two samples of data, each of 3000 samples drawn from a $N(30,10)$ distribution that is then limited to a lower range of 5 and upper range of 50 (*i.e.* using np.min/np.max). These samples represent the ages of a random population. Call the first sample `age18` and create a second set named `age0` that is `age18-18`. Then create a vector `is_adult` that is `1` for each `age18` entry that is $\\geq 18$ and 0 for all other entries.\n",
    "\n",
    "In other words, both samples contain the same data, but `age0` is shifted by `18`. The `is_adult` vector encodes the same information for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d070d095cd3bcc2470a7210ff65f40d9",
     "grade": true,
     "grade_id": "cell-60c450c3c85512ab",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28.90149795 19.90378454 33.53674475 25.07094215 13.90274805 35.01643056\n",
      " 28.15098817 50.         29.20954312 42.10609369 18.95057838 38.6347019\n",
      " 27.22327803 31.36606585 29.47679792 30.69419131 33.93236436 22.45177005\n",
      " 35.04514238 25.99165179 35.50352396 39.1680846  23.01403258 18.60377039\n",
      " 21.85826054 22.4727524  38.33264685 26.44562234 36.93394698 17.47304032]\n",
      "3000\n",
      "2659\n"
     ]
    }
   ],
   "source": [
    "normal = np.random.normal(loc = 30, scale = 10, size=3000)\n",
    "atleast5 = np.maximum(np.full((1,3000),5)[0], normal)\n",
    "age18 = np.minimum(atleast5, np.full((1,3000),50)[0])\n",
    "print(age18[:30])\n",
    "age0 = age18-18\n",
    "is_adult = age18 >= 18\n",
    "is_adult = is_adult.astype(int)\n",
    "print(len(is_adult))\n",
    "print(sum(is_adult))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a logistic regression to the `age18` and `is_adult` dataset and print out the confusion matrix resulting from predicting the results for the `age18` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b7309b43f8f17406f72e4c7d8b03265",
     "grade": true,
     "grade_id": "cell-527316bb6ac9c87b",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[28.90149795 19.90378454 33.53674475 ... 23.20173184  5.02327642\n 10.56759602].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fa2ec0660d99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_adult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[1;32m   1345\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    638\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[28.90149795 19.90378454 33.53674475 ... 23.20173184  5.02327642\n 10.56759602].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "logmod = sklearn.linear_model.LogisticRegression().fit(age18, is_adult)\n",
    "logmod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a logistic regression to the `age0` and `is_adult` dataset and print out the confusion matrix resulting from predicting the results for the `age0` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ab81ae6d7c94b9edb310f8a25cc68148",
     "grade": true,
     "grade_id": "cell-1050c91a4f144f40",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the two confusion matricies, comment on their relationship to each other and their ability to predict the target data. Note that since the sample data is stochastic, you may want to run the code a few times to determine a common pattern between the confusion matricies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "48fc49f278de1e5ceb42c6a30ed1a1b1",
     "grade": true,
     "grade_id": "cell-cd91b391b4241cd4",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help understand these results, you will prepare two plots of the `logit` plot. In the first plot, you should plot the logitistic function using the intercept and parameter from fitting the `age18` data. In the second plot, you should plot the logitstic function using $b_0 = 0$ and varying $b_1$ from 1 to 40 --  you only need 3 or 4 values in that range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0da43b39d0328d79040f18091a56bdb1",
     "grade": true,
     "grade_id": "cell-ee202fe24afc681a",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those two plots and the two confusion matricies, explain the results from the logistic classification for `age18` and `age0`. Comment on the trend shown in the second graph of the ability of the logit to separate the values less than zero from those greater than zero; what value would likely result in a perfect separate? What happens when you re-run your logistic classification using 300,000 samples rather than 3,000?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b66fe8559a06ab655f07bba046574f7c",
     "grade": true,
     "grade_id": "cell-0d681ab33b1aafb4",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Surviving the Titantic [30 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a159f71fab6722b387e1aa3769f0f29e",
     "grade": false,
     "grade_id": "cell-9d10eb6c3cdbef13",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ti = pd.read_csv('https://raw.githubusercontent.com/jorisvandenbossche/pandas-tutorial/master/data/titanic.csv')\n",
    "print(ti.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, build a logistic classification model that maximizes the accuracy of predicting who survives the Titantic. You can use the Patsy tool to prepare the design matrix. The underlying survival rate is 0.40 and your model should achieve an accuracy of 0.80 or greater. You should print out your prediction accuracy.\n",
    "\n",
    "In practice, you would split your data into training and testing data, but for this first set you should train and test on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "20b933be3848db0fd0385c76f62428ed",
     "grade": true,
     "grade_id": "cell-abf66a5abbedcb0d",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using your existing logistic design matrix, split the dataset into a train and test subset. The training set should use the even data elements and the testing set should use the odd. [You can easily construct the even/odd sets using numpy indexing.](https://stackoverflow.com/questions/4988002/shortest-way-to-slice-even-odd-lines-from-a-python-array) We use even/odd rather than `sklearn's train_test_split` function because it produces predictable output letting us compare multiple homework solutions. In practice, you would do something more robust.\n",
    "\n",
    "Re-run your regression model and report the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8ba82823353739811bd473914771cfb4",
     "grade": true,
     "grade_id": "cell-85e318a158257854",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the K-Nearest Neighbors classification method to predict the survival rate and print out the prediction accuracy. You should do this first using the full dataset. Vary the $k$ to find the smallest $k \\in \\{2,3,4,5\\}$ that maximizes the accuracy. Remember that you may want to use a different model than for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6b0d7afa6ab09b79118538a28c65c525",
     "grade": true,
     "grade_id": "cell-871c5475ed2ca22a",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the K-Nearest Neighbors classification method to predict the survival rate and print out the prediction accuracy. You should now do this **using the test & train sets**. Vary the $k$ to find the smallest $k \\in \\{1,2,3,4,5\\}$ that maximizes the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "43ce38849a40ce98cbf497ebc2dc1f3c",
     "grade": true,
     "grade_id": "cell-85eb6f9ba4a8f08f",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skim through [this paper that analyzes the survival information concerning the titantic](https://www.sciencedirect.com/science/article/pii/0277953686900419?via%3Dihub). That paper uses z-test comparisons (because $n$ is \"large\") to compare survival rates between different groups and uses the historical Mersey investigation to attribute reasons behind the differences.\n",
    "\n",
    "Assuming that `sklearn` could easily produce effects tables such as Table 4.3 in ISLR, describe the benefit of the logistic classification technique compared to the KNN technique for a researcher such as Wayne Hall. Then, describe the benefit of KNN for other applications. Use the terminology of ISLR 2.1.1 (\"Why estimate f?\") in your discussion. Assuming similar accuracy performance, which would you use if you were trying to suggest what movie to watch rather than who survived the titantic?\n",
    "\n",
    "If you would like to compute an effects table, you can use the [`StatsModels` logit](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.html) library, but I found that it depended on an older version of `scipy.stats` and it's hard to make it work, although you can make it dump out the `pvalues` table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "79470dc8caa53f245ada45422c278ba8",
     "grade": true,
     "grade_id": "cell-671d37afabdb6883",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3. Image Recognition using Classification [40 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you're going to use classification methods such as Logistic, LDA and KNN to classify handwritten numbers. This problem was originally posed for the US Post Office and one of the data sets was assembled by the National Institute for Standards and Technology (NIST). The problem is now a standard problem in machine learning. We'll see that we can get ~92% accuracy for the full problem using Logistic or LDA and about 98% accuracy using KNN.\n",
    "You can also use TensorFlow to [implement a \"deep learning\" solution](https://www.tensorflow.org/versions/r1.1/get_started/mnist/pros) that achieves ~99% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Small Dataset\n",
    "\n",
    "We're going to focus on the trade-off of training time vs. prediction time and the accuracy achieved by different classification methods. We will first use a dataset that uses 1797 small 8x8 (64 pixel) images. In the code below, we load the dataset using an `sklearn` interface.\n",
    "\n",
    "In this section, your goal will be to understand how *multinomial* classification functions and how you can use outputs of (some) classification tools to understand how certain or confidence you should be in a classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fa53a713e1d74d71b8ee1c3abf3e5ee0",
     "grade": false,
     "grade_id": "cell-43f8927c070a50b5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(\"Image Data Shape\" , digits.data.shape)\n",
    "print(\"Label Data Shape\", digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is encoded as values between 0 and 16. When training the data, we view the image as a 64-element set of features or factors. We can view the image by arranging it an 8x8 array and using [pyplot's `imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) routine, as we do below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=33\n",
    "plt.imshow(digits.data[k].reshape(8,8), cmap=plt.cm.gray)\n",
    "print('Digit:', digits.target[k])\n",
    "print('Image:\\n', digits.data[k].reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, divide the `digits` dataset into a train/test split using even/odd images as before. Again, we do this to allow precise comparison o the results betwen solutions and students. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_train, dX_test, dy_train, dy_test = digits.data[0::2], digits.data[1::2],digits.target[0::2], digits.target[1::2]\n",
    "print(dX_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Digits using Logistic\n",
    "\n",
    "Use a logistic classifier fit with the training data to then predict the test data. Report the accuracy score and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5ed054fc7603024b47c730341a3c690e",
     "grade": true,
     "grade_id": "cell-e4d1cbd0bc490255",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the test data, which pairs of digits are confused more than once? In other words, if you examine the first column, you see 2 predictions where a '0' is misclassified as a '4'; you would report this as {0,4}. Construct similar sets of confused digits for all entries confused more than 1 time. Comment on the any expected and suprising outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "744c803b5f63e2bcd405eee454196605",
     "grade": true,
     "grade_id": "cell-8c3cbc039425e33a",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digit classification problem involves a *multinomial*, or more than two levels in the outcome. By default, the `LogisticRegression` method uses a series of binomial logistic regression fits to the different outcomes of the multinomial. [The `predict_proba` routine in `LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) returns the probability of the fit to each individual possible outcome (e.g. the digits '0' through '9'). The predicted outcome (i.e. the result of `predict`) is then the outcome with the largest predicted outcome.\n",
    "\n",
    "For the two examples where the predicted digit is '4' but the actual digit is '0', plot the images corresponding to those digits and print out the results of `predict_proba` for those targets. In my solution to this, produced a vector of True/False values using element-wise comparisons and then [used `np.nonzeros`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) to extract the indicies in the test data of the \"true\" values (corresponding to the samples that matched '4' in my prediction but whose actual target was '0'). you may also want to use `np.round` to round up the `predict_proba` results to 3-4 digits, making it eaiser to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a24e23077d76e5173adbb3a80d9f92c3",
     "grade": true,
     "grade_id": "cell-beef05ea87ea1fe0",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the values from `predict_prob` are both mis-classified '0' values equally likely to have been classified as a '4'? Do the probabilities of the predicted outcomes comport with your visual interpretation of the digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c2e2b878194a817c5ea2ce2e218f1798",
     "grade": true,
     "grade_id": "cell-f23e7955ae9eefe5",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Digits using KNN\n",
    "\n",
    "Now, using the K-Nearest Neighbors method to fit and predict the test and train data. Select $k \\in \\{1,2,3,4,5\\}$ that achieves the highest accuracy. Print the accuracy score and confusion matrix for the $k$ with the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54371573565ded21e3a3e9b8627ed3cc",
     "grade": true,
     "grade_id": "cell-887950113246dc76",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, based on the test data, which pairs of digits are confused more than once?  Comment on the any expected and suprising outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "234fb5ab56ae26e41edf965d029098e3",
     "grade": true,
     "grade_id": "cell-490d026115d69b2a",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting one pair of confused digits, print out the image and the probability estimates (`predict_proba`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "712f56b5054bbadd30ea2c9a254645d6",
     "grade": true,
     "grade_id": "cell-30ab77e0094c72eb",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on differences in the results of `predict_proba` between the logistic and KNN classifiers. Would the results be similar for different values of $k$ in the KNN sarch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "18bb09c9f4b25a203f6a5cd8c355f75f",
     "grade": true,
     "grade_id": "cell-f99b893a2d0e851e",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the larger MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the MNIST dataset, which is the same used in the TensorFlow tutorial. This dataset is large, and contiains 70,000 images each of which are 28x28 pixels.\n",
    "\n",
    "In this section, your goal will be to understand the performance of difference classification tools and their impact on usability in an application.\n",
    "\n",
    "We first load the dataset. This may take a while the first time because the data has to be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "mnist = sklearn.datasets.fetch_mldata('MNIST original')\n",
    "print(\"Image Data Shape\" , mnist.data.shape)\n",
    "print(\"Label Data Shape\", mnist.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the dataset has a `data` array of 784 features or factors that can be reorganized into an image. There is also a `target` value indicating the correct digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "plt.imshow(np.reshape(mnist.data[k], (28,28)), cmap=plt.cm.gray, label='Digit:' + str(mnist.target[k]))\n",
    "print('values:', mnist.data[k].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, split your data into an even/odd train/test dataset using numpy indexing. You should name the data something different than your smalled 'digits' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mX_train, mX_test, my_train, my_test = mnist.data[0::2], mnist.data[1::2],mnist.target[0::2], mnist.target[1::2]\n",
    "print(mX_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train a logistic regression model on the MNIST training data. You [should prefix your fit function call using the %time \"magic\" command](http://ipython.readthedocs.io/en/stable/interactive/magics.html) to measure how long the fitting process takes. \n",
    "\n",
    "This will take a long time for the default method we've been using to run logistic classification problems (like more than 30 minutes), in part because the default method fits $n$ binomial classification problems to determine the multinomial model. If you start using the standard solver (`liblinear`) and decide it's too slow, use the Kernel -> Interrupt menu to stop the evaluation.\n",
    "\n",
    "Logsitic regression uses *maximum liklihood estimation* to determine the most likely outcome. There are numerous *solvers* (see [the LogisticRegression manual](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) ) that can be used and some of them are more appropriate for large multinomial problems because they fit the data to all the outcomes in one go. Find one that doesn't take forever (some should take ~15 seconds) and fit your model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a2f1c17efc6aad3ea87a3f6f27463a7e",
     "grade": true,
     "grade_id": "cell-b3b79e2fdb361e3b",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the predictions and `predict_proba` for the test dataset and use %time to determine how long the predictions take. Report the accuracy score and the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c0349a285581070132cfee737183ce5f",
     "grade": true,
     "grade_id": "cell-540891cbf1ad3948",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the probability scores for each outcome class using `predict_proba` and plot either a histogram or KDE plot of their values. You can use the output of `predict_proba` and then use `ravel()` to turn it into single flat array suitable for feeding to `plt.hist` or `sns.kdeplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0470fcbe87ab27f564755386e5ce2457",
     "grade": true,
     "grade_id": "cell-5e6352ef5820b2b0",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis is supposed to be superior for multinomial classification. Run the same classification problem using LDA and time the fitting proccess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c1e39a6a15cbf8d5f6bda0172132575a",
     "grade": true,
     "grade_id": "cell-3773098cbc5a5acd",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the outcomes and report the accuracy score and confusion matrix. Time how long it takes to run the prediction using `%time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ed8327c44657a60da2c266f33401cb94",
     "grade": true,
     "grade_id": "cell-b85801f7aceb3996",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the distribution of outcome probabilities from `predict_proba` using a histogram or KDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c47576af94abf5a03cdff75d17bfa361",
     "grade": true,
     "grade_id": "cell-c78a3b59ed23429c",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the distribution of probability of prediction values for Logistic and LDA classification. Comment on the differences and/or similarities of the range of values from `predict_proba` returned by each method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8f7a89ee79d0d18c5b2fcf580f615057",
     "grade": true,
     "grade_id": "cell-911ebd9f0a8eb177",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we're going to do the same steps using the KNN algorithm. You should use $k=1$ for the KNN method and record the fitting time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3bf8d0e202e81b4fcca17a89e85a0d21",
     "grade": true,
     "grade_id": "cell-1ef891ebd246c7b1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the prediction using your KNN model. Note that this will take a long time (40 minutes?). If specify `n_jobs=-1` when you create your `KNeighborsClassifier`, then predictions will use all the cores on your computer. For example, that chnaged my 40 minute run time for the full dataset to 5 minutes.\n",
    "\n",
    "You should first run the prediction on a small test set (e.g. perhaps every 40th sample) to make certain you're doing it right. The digits of the same outcome are usually bunched together and if you just e.g.select the first 1000 items, you'll find they only belong to one output class. Once you have your code working, run it for the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5bb2281e3d1efe512caf710ba9d504ad",
     "grade": true,
     "grade_id": "cell-8c4d728c3759826e",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "Now, compare the three methods. For each method, describe the accuracy achieved, the fitting time and the prediction time. For the Logistic and LDA model, describe how the distribution of the outcome probabilities may affect the accuracy score. Assume you're trying to apply the digit classification problem in the post-office. Which method would you use? Given that the accuracy isn't 100%, what outputs of the models could you use to improve mail sorting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3564777d0f2df2d36314923d1e933cec",
     "grade": true,
     "grade_id": "cell-1a0de55dff3e9630",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nteract": {
   "version": "0.14.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
